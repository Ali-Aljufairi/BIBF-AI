# Bert

The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Itâ€™s a bidirectional transformer pre-trained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.

![](attachments/Pasted%20image%2020250115233039.png)

# Sentence Transformers

SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. It provides a simple interface for computing embeddings for sentences, paragraphs and images.

# AI vs Gen Ai

Gen Ai is a subset of AI that focuse creating new content of data that is novel and 
realstie


# Positional Encodings 
![](attachments/Positional%20Encoding%20in%20Transformers.png) 

# Transoformer Archticure




![](attachments/Understanding%20Transformer%20Architecture%20AI%20(1)%201.png)


#  Tokenization NLP



![](attachments/Tokenization%20in%20NLP%20TeacherSeat%20(1).png)



# Foundational Model

![](attachments/Foundational%20Model%20ExamPro.png)



